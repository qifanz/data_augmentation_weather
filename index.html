<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Computer Vision Class Project</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="css/bootstrap.css" rel="stylesheet">
    <link href="css/custom.css" rel="stylesheet">
    <link href="css/bootstrap-responsive.min.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
    <div class="container">
        <div class="page-header">

            <!-- Title and Name -->
            <h1 style='text-align: center'>Data augmentation - different weathers</h1>
            <p style="text-align: center; font-size: 20px; line-height: 1.5em;"><strong>Eric Gastineau, Yiliang Guo, Weiguang Huang, Qifan Zhang</strong></p>
            <p style="text-align: center; font-size: 18px; line-height: 1.5em;">Fall 2019 Computer Vision: Class Project</p>
            <p style="text-align: center; font-size: 18px; line-height: 1.5em;">Georgia Tech</p>
            <hr>

            <h2>Abstract</h2>
            <p>
                The project will focus on applying computer vision techniques to effectively synthesize images with various types of weather conditions, such as rainy days, foggy days or snowy days. The goal is to transform and synthesize images based on original images
                to effectively attain extra image data for deep learning model training. Techniques considered to be applied include color mapping, image segmentation, texture modifications and other transformations.
            </p>
            <hr>

            <h2>Project Proposal</h2>
            <h3>I. Problem statement</h3>
            <p>
                Deep learning techniques require a large amount of labelled data of good quality. Many widely recognized datasets are used to train models for autonomous vehicles like Cityscape, BDD, AppolloScape etc.. Unfortunately, the majority of images from these
                datasets are taken during days with good weather. As a result, most models trained on these datasets can work well in good weather conditions, but deteriorate significantly in other weather conditions, for example during a foggy day.</p>
            <p>
                Our solution for this problem is to perform data augmentation before model training<sup>[4]</sup>. The goal of the project is to change the weather conditions in the images which are originally taken in good weather and already labelled,
                and to produce labelled dataset with various weathers (with snow, smog, or rain etc.).
            </p>
            <div class='teaser'>
                <img class='teaser-img' src="img/teaser1.jpg">
            </div>
            <div class='teaser'>
                <img class='teaser-img' src="img/eiffel_tower.png">
                <br> Eiffel tower in different weathers<sup>[7]</sup>
            </div>

            <hr>
            <h3>II. Approach</h3>
            <p>
                Due to technical difficulties, and because we would like to create data augmentation for autonomous vehicles, we decide to first focus on Road Driving images.
                <br>In this project, we chose to use two types of methods: traditional computer vision and deep learning.
            </p>
            <h4>
                1. Traditional version
            </h4>
            <p>
                In the traditional version, we first do segmentation on a road driving image that we would like to change the weather and create augmented images. We then extract textures of “weathers” from target image. Finally we do texture transfer on each segment
                of original image to create an augmented image.

            </p>
            <div class='teaser'>
                <img class='teaser-img' src="img/pipeline.png">
                <br> Pipeline for traditional method
            </div>

            <p>
                We tested a range of algorithm on the segmentation task to separate the background scene of the images and got to know better the response from the algorithms on the task. The main difficulty in our approach is to clearly separate the road and the background,
                especially trees and other plants. For this reason, after several manual tests, we decided to choose K-means.
            </p>
            <p>
                An implementation option of texture transfer is as the algorithm described in the paper Image Quilting for Texture Synthesis and Transfer (referred to as Quilting Paper). Reference implementation code for the algorithm is published on Github.

            </p>
            <div class='teaser'>
                <img class='teaser-img' src="img/texture_transfer.png">
                <br> Texture transfer example, from paper <i>Image Quilting for Texture Synthesis and Transfer</i>
            </div>
            <p>
                However, the algorithm in Quilting Paper doesn’t adapt well to synthesis naturally looking image across distinctive segments in a road image if simply replacing textures in selected segments. Rather, we further adapt the basic thoughts of the algorithm
                but modify the particular method of texture generation and synthesis. The process is conducted as:

            </p>
            <p>
                1. Segments(k-mean based) the road image
                <br> 2. Find the edges (Canny detector based) of the segments
                <br> 3. In each segment (with respect to edges), select areas by certain adjustable distribution (different from the Quilting Paper which select continuous square areas), also respect the edges in a degree adjustable.
                <br>4. For the areas selected, synthesize at pixels of the road image and the weather image (different from the Quilting Paper which synthesize at square areas base). The weather image serve as a texture example.

            </p>
            <h4>
                2. Deep learning version
            </h4>

            For Deep Learning part, there are 2 approaches to use : <u>Deep Style Transfer</u> and <u>CycleGAN</u>.

            <h5>
                Deep Style Transfer
            </h5>

            <p>
                For Deep Style Transfer, the goal is to transfer the “style” of an image to another image. Thus we need at least one input image ( the image we want to transform ) and one style image. This technique has been widely used for artistic purposed.
            </p>
            <p>
                Our idea was to use this approach to change the weather of a picture. Particularly, we wanted to use this method to add snow in pictures. A recent version of Deep Style transfer use segmentation in order to transfer the textures to each part of the image.
                Our idea was to take 2 images ( one from a sunny day and the other from a snowy day ) that are closely related to efficiently do the transfer.
            </p>
            <p>
                The code we used was the official code from the original paper that we modified to fit our problem. In this part, we assume the segmentation of the picture has been done manually so that we can focus on the style transfer. <i>TODO: we would like to automatize this.</i>
            </p>

            <h5>
                Cycle GAN
            </h5>
            <p>
                The goal of GANs is to produce realistic images. With CycleGAN, we want to transform an image from one domain A ( for instance sunny days ) to another domain B ( for instance snowy days ). We are not going to expand on the whole theory, but roughly this
                technique use CNN to do this conversion. There are 2 CNNs G and F used to transform the images ( one that transform from A to B and the other from B to A ) and 2 CNNs Dx and Dy used to discriminate between the 2 domains. The cycle consistency
                between the 2 domains and the discriminators scores are used to optimize these networks
            </p>
            <div class='teaser'>
                <img class='teaser-img' src="img/cycle_theory.png">
                <br>
            </div>

            <hr>


            <h2>Experiments and results</h3>
                <h3>
                    I. Program Code
                </h3>
                <p>
                    Overall, we used Python as programming language.
                    <br> In traditional part, we used Scikit-image library for k-means algorithm. For the rest parts, we implemented by ourself.
                    <br> In deep learning part, we used Pytorch framework.
                </p>
                <h3> II. Performance Evaluation and Validation</h3>
                The evaluation of the two different methods can be performed on two different aspects accuracy, which is the possibility to be considered as a “correct class” image and speed of augmentation.
                <br> For accuracy, the simplest and qualitative way of evaluation is to validate it using human’s eyes and judge it as good or bad. A better and a qualitative solution is to train a classification neural network, and see if augmented images
                using either GAN or classical image synthesis method can be classified into correct classes.
                <br> We first trained a simple classification neural network using 50 images per class (normal day, foggy day and snowy day). Given the simplicity of task the network achieved around 100% validation accuracy. Thus, it is ready to validate
                our result quantitatively.


                <h4>1. Algorithmic Performance Evaluation </h4>
                We augmented 50 images with sunny weather condition into 50 with foggy weather condition and 50 with snowy weather condition. These images were then fed to the classification network and measured in the classification accuracy as following.

                <h5>Classification Accuracy for Datasets Generated by GAN and Texture Synthesis</h5>
                <table style="width:100%;">
                    <tr>
                        <th></th>
                        <th>Foggy</th>
                        <th>Snowy</th>
                        <th>Time used</th>
                    </tr>
                    <tr>
                        <td>GAN</td>
                        <td>100%</td>
                        <td>90%</td>
                        <td>N/A</td>
                    </tr>
                    <tr>
                        <td>Texture synthesis</td>
                        <td>100%</td>
                        <td>98%</td>
                        <td>271 seconds for snowy, 1206 seconds for foggy</td>
                    </tr>

                </table>
                We can see that due to image consistency of foggy weather condition, both methods achieved perfect accuracy. For augmented images data for snowy weather condition, traditional method with texture synthesis beats GAN slightly. Further explanation is discussed in next
                part, qualitative analysis. <br>In the comparison of computation speed, traditional method is much faster, and without the need and time cost to train a GAN network. Overall, we conclude that in the weather condition augmented task, traditional
                method a better solution.


                <h5>Comparison of Different segmentation algorithms used in traditional approach</h5>
                <p>
                    <b>Score criteria</b>
                    <br>0: failed to separate the background and foreground
                    <br>1: separate the background and foreground, but fail to separate in reasonable boundary
                    <br>2: separate road, trees, sky, but have significantly mistake in boundary
                    <br>3: separate road, trees, sky correctly, but have noticeable boundary mistakes
                    <br>4. Separate road, trees, sky correctly, have mistakes in secondary boundary within the major segments
                    <br>5. Separated road, trees, sky correctly, do well in secondary boundary within major segments
                </p>
                <table style="width:100%;">
                    <tr>
                        <th>Algorithm Category</th>
                        <th>Characteristic</th>
                        <th>Total performance score</th>
                    </tr>
                    <tr>
                        <td>Denoise + kmean</td>
                        <td>Relatively good performance at segmenting the major segments of road driving image</td>
                        <td>46</td>
                    </tr>
                    <tr>
                        <td>Denoise + kmean + thresholding with normalized cut</td>
                        <td>Easily mistake at boundaries of major segments</td>
                        <td>38</td>
                    </tr>

                    <tr>
                        <td>
                            Denoise + water thresholding
                        </td>
                        <td>Relatively good performance at segmenting major segments and secondary boundaries within larger segment</td>
                        <td>62</td>
                    </tr>

                </table>
                <h4>
                    2. Qualitative Analysis of Algorithm Families
                </h4>
                <h5>1. Deep Style Transfer</h5>
                <p>
                    Firstly, we note that the edges of the road are tend to be blurring. The reason is that the segmentation is not perfect and some texture from the snow is applied to the road.
                    <br> Secondly, we see frequently the problems of “averaging” effect. The snow on the trees is sometime gray which is not realistic. The reason is that the algorithm applies a loss minimization, and that as
                    the two main colors in the style image are black and white, the solution of the minimization tends to be gray differing from real snow.
                    <br> Finally, the snow on the roadside is hardly realistic. The snow is merely white in gradient without texture. We don’t see the granularity of the snow that would make it more realistic.
                    <br> The overall result of this method does not match our expectations since it is tested on a relatively simple problem (a road surrounded by trees). The performance would degrade even more in real more complex road situations such as around residential area.
                </p>

                <div class='teaser'>
                    <img class='teaser-img' src="img/result/deep/transfer.png">
                    <br> Deep Style Transfer result 1
                </div>
                <h5>2. Cycle GAN</h5>
                <p>
                    We used CycleGAN for 2 different domain transfer : Sunny and Foggy bi-directional, and Sunny to Snowy bi-directional.
                    <br> Unfortunately, there was a lack of public dataset available for these two types of domain transfer. The images we used are a mix from the BDD dataset and from Google Image Search.
                    <br> For Sunny and Foggy bi-directional we used 205 images from sunny days and 205 images from foggy days.
                    <br> For Sunny and Snowy bi-directional we used 295 images from sunny days and 293 images from snowy days.
                    <br> The code we used is the official code from the original paper and was modified to fit our problem.
                    <br> For Sunny and Foggy bi-directional, we had promising results.
                    <br> Image examples transferred from Sunny and Foggy bi-directional: (sunny input from left, artificial foggy output to right)

                </p>

                <div class='teaser'>
                    <img class='teaser-img' src="img/result/deep/cycle1.png">
                    <br>
                </div>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/deep/cycle2.png">
                    <br>
                </div>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/deep/cycle3.png">
                    <br>
                </div>
                <p>
                    Some degradations exists in the conversion but a realistic fog can be clearly seen.
                    <br> However, the reverse conversion from Foggy to Sunny is harder because the loss of information is has been in the first pass of conversion :

                </p>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/deep/cycle4.png">
                    <br>
                </div>

                <p>
                    It is very challenging for the neural network to invent features not existing, but it is not our focus as our initial goal is conversion from sunny days.
                    <br> For Sunny and Snowy bi-directional, the results are less promising : (sunny input from the left, artificial snowy output to the right)

                </p>

                <div class='teaser'>
                    <img class='teaser-img' src="img/result/deep/cycle5.png">
                    <br>
                </div>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/deep/cycle6.png">
                    <br>
                </div>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/deep/cycle7.png">
                    <br>
                </div>

                <p>
                    The snow is correctly applied to trees (even sometimes with problems of averaging). The biggest problem is the coloring of the road. The issue is caused by that many images in the dataset have roads either covered by snow or de-iced.
                    The above conditions constrain the neural network in effectively transferring for the roads. In many images, the neural network simply applied an averaged unrealistic color between white and gray, and in other images wildly different colors as presented
                    above. The Sunny and Snowy bi-directional conversion is certainly the more difficult since a single neural network has limited capability to effectively handle the various situations along with the snowy weather conditions.
                    <br> As with the foggy weather, the information lost in the first pass direction, and thus the reverse conversion from snowy to sunny is more difficult:

                </p>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/deep/cycle8.png">
                    <br>
                </div>


                <h5>3. Traditional Approach</h5>
                Here are some results generated by segmentation + texture synthesis:
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/trad/4_s.jpg">
                    <br>
                </div>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/trad/23_s.jpg">
                    <br>
                </div>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/trad/44_f.jpg">
                    <br>
                </div>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/trad/49_f.jpg">
                    <br>
                </div>
                We can see these augmented images are of good qualities. However in some cases it may fail.
                <br>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/trad/25_s.jpg">
                    <br>
                </div>
                In this failed example, we should have cropped the top and bottom part of the sunny image before synthesis because the original sunny image has dark frame/banners on top and bottom.
                <br> Some pre-process such as frame edge detection could be implemented to crop the dark frames to improve the image quality.

                <h5>4. Segmentation algorithms</h5>
                <br><u>Kmean worked example</u>
                <div class='teaser'>
                    <img class='teaser-img' src="img/result/trad/kmeans.png">
                    <br>
                </div>
                <br><u>Kmean failed example</u>
                <div class='teaser'>
                    <img class='teaser-img-big' src="img/keman_fail.png">
                    <br>
                </div>
                <br><u>Kmean + threshold failed example</u>
                <div class='teaser'>
                    <img class='teaser-img-big' src="img/kmean_thresh_fail.png">
                    <br>
                </div>
                <br><u>Water thresholding failed example</u>
                <div class='teaser'>
                    <img class='teaser-img-big' src="img/water_fail.png">
                    <br>
                </div>


                <hr>
                <h2>
                    Reference
                </h2>

                <p>
                    1. Perez, Luis, and Jason Wang. "The effectiveness of data augmentation in image classification using deep learning."
                    <i>arXiv preprint arXiv:1712.04621 (2017)</i>
                </p>

                <p>
                    2. Takahashi, Ryo, Takashi Matsubara, and Kuniaki Uehara. "Data augmentation using random image cropping and patching for deep cnns."
                    <i>IEEE Transactions on Circuits and Systems for Video Technology (2019)</i>
                </p>

                <p>
                    3. https://bdd-data.berkeley.edu/
                </p>

                <p>
                    4. DConnor Shorten and Taghi M. Khoshgoftaar. "A survey on Image Data Augmentation for Deep Learning."
                    <I>Shorten and Khoshgoftaar J Big Data (2019) 6:60   </I>
                </p>
                <p>
                    5. Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala. "Deep Photo Style Transfer"
                    <i>arXiv preprint arXiv:1703.07511</i>
                </p>
                <p>
                    6. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros. "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"
                    <i>arXiv preprint arXiv:1703.10593</i>
                </p>
                <p>
                    7. Wei-Ta Chu, Xiang-You Zheng, Ding-Shiuan Ding. "Image2Weather: A Large-Scale Image Dataset for Weather Property Estimation"
                    <i>2016 IEEE Second International Conference on Multimedia Big Data (BigMM)</i>
                </p>
</body>

</html>