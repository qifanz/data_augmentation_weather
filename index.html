<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
  <link href="css/custom.css" rel="stylesheet">
  <link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1 style='text-align: center'>Data augmentation - different weathers</h1> 
<p style="text-align: center; font-size: 20px; line-height: 1.5em;"><strong>Eric Gastineau, Yiliang Guo, Weiguang Huang, Qifan Zhang</strong></p>
<p style="text-align: center; font-size: 18px; line-height: 1.5em;">Fall 2019 Computer Vision: Class Project</p>
<p style="text-align: center; font-size: 18px; line-height: 1.5em;">Georgia Tech</p>
<hr>

<h2>Abstract</h2>
<p>
  The project will focus on applying computer vision techniques to effectively synthesize images with various types of weather conditions, such as rainy days, foggy days or snowy days. 
  The goal is to transform and synthesize images based on original images to effectively attain extra image data for deep learning model training. 
  Techniques considered to be applied include color mapping, image segmentation, texture modifications and other transformations.
</p>
<hr>

<h2>Project Proposal</h2>
<h3>Problem statement</h3>
<p>
  Deep learning techniques require a large amount of labelled data of good quality. 
  Many widely recognized datasets are used to train models for autonomous vehicles like Cityscape, BDD, AppolloScape etc.. 
  Unfortunately, the majority of images from these datasets are taken during days with good weather. 
  As a result, most models trained on these datasets can work well in good weather conditions, but deteriorate significantly in other weather conditions, for example during a foggy day.</p>
<p>
  Our solution for this problem is to perform data augmentation before model training<sup>[4]</sup>. 
  The goal of the project is to change the weather conditions in the images which are originally taken in good weather and already labelled, 
  and to produce labelled dataset with various weathers (with snow, smog, or rain etc.).
</p>
<div class='teaser'>
  <img class='teaser-img' src="img/teaser1.jpg">
</div>
<div class='teaser'>
  <img class='teaser-img' src="img/eiffel_tower.png">
  <br>
  Eiffel tower in different weathers<sup>[7]</sup>
</div>


<h3>Approach</h3>
<p>
  In this project, we chose to use two types of methods: traditional computer vision and deep learning.
</p>
<h4>
  Traditional version
</h4>
<p>
  In the traditional version, we first do segmentation on a road driving image that we would like to change the weather and create augmented images.
  We then extract textures of “weathers” from target image. Finally we do texture transfer on each segments of original image to create an augmented image. 
</p>
<div class='teaser'>
  <!--img class='teaser-img' src="img/teaser1.jpg"-->
  <br>
  Pipeline for traditional method
</div>

<p>
  We tested a range of algorithm on the segmentation task to separate the background scene of the images and got to know better the response from the algorithms on the task. The main difficulty in our approach is to clearly separate the road and the background, 
  especially trees and other plants. For this reason, after several manual tests, we decided to choose K-means.
</p>
<p>
  In order to then do texture transfer, we aim to implement the algorithm described in the paper Image Quilting for Texture Synthesis and Transfer. 
  For example, we take an image containing texture of trees with snow and we transfer that to a new image. We currently have not yet finished this part due to difficulties in code.
</p>

<h4>
  Deep learning version
</h4>

For Deep Learning part, there are 2 approaches to used : <u>Deep Style Transfer</u> and <u>CycleGAN</u>.

<h5>
  Deep Style Transfer
</h5>

<p>
  For Deep Style Transfer, the goal is to transfer the “style” of an image to another image. Then we need at least one input image ( the image we want to transform ) and one style image. 
  This technique has been widely used for artistic purposed.
</p>
<p>
  Our idea was to use this idea to change the weather of a picture. Particularly, we wanted to use this method to add snow in pictures. 
  A recent version of Deep Style transfer use segmentation in order to transfer the textures to each part of the image. Our idea was to take 2 images 
  ( one from a sunny day and the other from a snowy day ) that are closely related to efficiently do the transfer. 

</p>
<p>
  The code we used was the officiel code from the original paper that we modified to fit our problem.
  In this part, we assume the segmentation of the picture has been done manually so that we can focus on the style transfer. <i>TODO: we would like to automatize this.</i>
</p>

<h5>
  Cycle GAN
</h5>
<p>
  The goal of GANs is to produce realistic images. With CycleGAN, we want to transform an image from one domain A ( for instance sunny days ) 
  to another domain B ( for instance snowy days ). We are not going to expand on the whole theory, but roughly this technique use CNN to do this conversion. 
  There are 2 CNNs G and F used to transform the images ( one that transform from A to B and the other from B to A ) and 2 CNNs Dx and Dy used to discriminate between the 2 domains. 
  The cycle consistency between the 2 domains and the discriminators scores are used to optimize these networks
</p>


<h3>Experiments and results</h3>
<h4>
  Program code
</h4>
<p>
  For our computer vision techniques for data augmentation, we will use open source library such as OpenCV to implement image segmentation, 
  texture fusion etc, and further creatively adapt these techniques to our dataset augmentation.
</p>

<p>
  For the controlled group of deep learning techniques, we will use existing and publically available model codes to train on our datasets. 
  We may adapted available codes to fit in our purpose. 
</p>

<h4>
  Dataset
</h4>
<p>
  The goal is to perform data augmentation on benchmark datasets such as Berkeley Deep Drive (BDD)<sup>[3]</sup>. 
  We may also have to use extra images outside of a dedicated dataset since no publicly recognized dataset that contains sufficient image data with certain weather conditions, 
  for example foggy day,  to enable a comprehensive analysis.
</p>

<h4>
  Result validation
</h4>

<p>
  The results will first be validated by human vision. A synthesized/augmented image will receive a high rate if it can’t be told from a group of other originally taken images. 
</p>

<p>
  In a computational way, the validation will be to train models with these synthesized/augmented images and test whether the models trained achieve a performance level same as or higher than trained with original images.
</p>

<hr>
<h2>
Reference
</h2>

<p>
  1.	Perez, Luis, and Jason Wang. "The effectiveness of data augmentation in image classification using deep learning."
   <i>arXiv preprint arXiv:1712.04621 (2017)</i>
</p>

<p>
  2.	Takahashi, Ryo, Takashi Matsubara, and Kuniaki Uehara. "Data augmentation using random image cropping and patching for deep cnns." 
  <i>IEEE Transactions on Circuits and Systems for Video Technology (2019)</i>
</p>

<p>
  3.	https://bdd-data.berkeley.edu/
</p>

<p>
  4.  DConnor Shorten and Taghi M. Khoshgoftaar. "A survey on Image Data Augmentation for Deep Learning."
  <I>Shorten and Khoshgoftaar J Big Data (2019) 6:60   </I>
</p>
<p>
  5.  Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala. "Deep Photo Style Transfer"
  <i>arXiv preprint arXiv:1703.07511</i>
</p>
<p>
  6.  Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros. "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"
  <i>arXiv preprint arXiv:1703.10593</i>
</p>
<p>
  7.  Wei-Ta Chu, Xiang-You Zheng, Ding-Shiuan Ding. "Image2Weather: A Large-Scale Image Dataset for Weather Property Estimation"
  <i>2016 IEEE Second International Conference on Multimedia Big Data (BigMM)</i>
</p>
</body></html>